{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is neccesary only if used spark in Windows\n",
    "# with the correct configuration\n",
    "\n",
    "#%pip install findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "experiment_name = \"mlflow-experiment-ny-taxis\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.spark.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Taxis Classifier to tripType\")\n",
    "        #.config(\"spark.jars.packages\", \"org.mlflow.mlflow-spark:1.11.0\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../data/taxis.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dimension's\n",
    "print((df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatypes of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "my_data = df.drop(*[\"_c0\", \"lpepPickupDatetime\", \"lpepDropoffDatetime\", \"ehailFee\"])\n",
    "my_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimensions of the data\n",
    "(my_data.count() , len(my_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sql function pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# null values in each column\n",
    "data_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])\n",
    "data_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts of columns\n",
    "for column in my_data.columns:\n",
    "    print(column, \":\")\n",
    "    my_data.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing steps\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "for column in my_data.columns:\n",
    "    SI = StringIndexer(inputCol=column, outputCol=column+'_Index', handleInvalid='skip')\n",
    "    my_data = SI.fit(my_data).transform(my_data)\n",
    "    print(column, \"transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the transformed data\n",
    "my_data.select(\"vendorID\", \"vendorID_Index\", \"tipAmount\", \"tipAmount_Index\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [column for column in my_data.columns if column.endswith('Index')]\n",
    "inputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCols = [col+\"_OHE\" for col in my_data.columns if not col.endswith('Index')]\n",
    "outputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object and specify input and output column\n",
    "OHE = OneHotEncoder(inputCols=inputCols, outputCols=outputCols)\n",
    "\n",
    "# transform the data\n",
    "my_data = OHE.fit(my_data).transform(my_data)\n",
    "\n",
    "# view and transform the data\n",
    "my_data.select('vendorID', 'vendorID_Index', 'vendorID_OHE','tipAmount','tipAmount_Index','tipAmount_OHE').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_vector =[val[0] for val in my_data.dtypes if val[1] != 'string' and not val[0].startswith(\"tripType\")]\n",
    "len(cols_to_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=cols_to_vector,\n",
    "                           outputCol='features')\n",
    "\n",
    "# fill the null values\n",
    "my_data = my_data.fillna(0)\n",
    "\n",
    "# transform the data\n",
    "final_data = assembler.transform(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the transformed vector\n",
    "final_data.select('features','tripType_Index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.groupby(\"tripType_Index\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data_agg = final_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])\n",
    "f_data_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model_Dataframe\n",
    "model_df = final_data.select(['features','tripType_Index'])\n",
    "model_df = model_df.withColumnRenamed(\"tripType_Index\",\"label\")\n",
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training & testing Dataframe\n",
    "training_df,test_df = model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a logistic regression model object\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "with mlflow.start_run():\n",
    "    log_reg=LogisticRegression().fit(training_df)\n",
    "    lr_summary=log_reg.summary\n",
    "    mlflow.log_metric(\"accuracy\",lr_summary.accuracy)\n",
    "    mlflow.log_metric(\"recall\",lr_summary.weightedRecall)\n",
    "    mlflow.log_metric(\"precision\",lr_summary.weightedPrecision) \n",
    "    mlflow.log_metric(\"f1\",lr_summary.weightedFMeasure())\n",
    "    mlflow.log_metric(\"area under ROC\",lr_summary.areaUnderROC)\n",
    "    mlflow.spark.log_model(log_reg, \"model\")\n",
    "\n",
    "#log_reg.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_summary=log_reg.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall accuracy of the classification model\n",
    "lr_summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area under ROC\n",
    "lr_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision of both classes\n",
    "print(lr_summary.precisionByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall of both classes\n",
    "print(lr_summary.recallByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Preditions\n",
    "predictions = log_reg.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('label','prediction').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    rf_clf = RandomForestClassifier().fit(training_df)\n",
    "    rf_summary=rf_clf.summary\n",
    "    mlflow.log_metric(\"accuracy\",rf_summary.accuracy)\n",
    "    mlflow.log_metric(\"recall\",rf_summary.weightedRecall)\n",
    "    mlflow.log_metric(\"precision\",rf_summary.weightedPrecision) \n",
    "    mlflow.log_metric(\"f1\",rf_summary.weightedFMeasure())\n",
    "    mlflow.log_metric(\"area under ROC\",rf_summary.areaUnderROC)\n",
    "    mlflow.spark.log_model(rf_clf,\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary = rf_clf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary.precisionByLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
